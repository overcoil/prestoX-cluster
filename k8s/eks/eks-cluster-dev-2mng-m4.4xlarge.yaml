# 
# This YAML is for eksctl, not kubectl!
#
# A barebone EKS cluster requires at least 2 subnets. For simplicity, go with public subnets to bypass any peering/config.
# Once the cluster is up, you'll probably want to rename the verbose kubeconfig context name. Then, it's ready for kubectl.
#
# ref: https://eksctl.io/usage/creating-and-managing-clusters/
#
# ref: https://www.qubole.com/blog/presto-performance-for-ad-hoc-workloads-on-aws-instance-types/
#
# This DEVELOPMENT cluster is configured as:
#    2 nodegroups
#    starting with 1 node in each nodegroup;
#    capped at 3 nodes max (1+2).
# of
#    m4.4xlarge: 16 vCPU/64 Gi RAM; USD0.80/h
#
# at an aggregate cost of:
#    min 2x0.80 = USD1.60/hr = ~CAD2/hr
#    max 3x0.80 = USD2.40/hr = ~CAD3/hr
#
# The role label is a Kubernetes label used via statefulset.spec.template.spec.affinity.nodeAffinity 
#
apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig

metadata:
  name: trino-feature-eval
  region: us-west-2

vpc:
  subnets:
    public:
      us-west-2a: { id: subnet-037e0c351390f3127 }
      us-west-2b: { id: subnet-044cee5348d4b3ea3 }
      us-west-2c: { id: subnet-00e02fb1b1db10fef }

managedNodeGroups:
  - name: managed-ngc
    labels: { role: px-coord }
    instanceType: m4.4xlarge
    desiredCapacity: 1
    minSize: 1
    maxSize: 1
    privateNetworking: false
    tags:
      bill-to: databricks
      billed-project: c3
      env: dev
      
  - name: managed-ngw
    labels: { role: px-work }
    instanceType: m4.4xlarge
    desiredCapacity: 1
    minSize: 1
    maxSize: 2
    privateNetworking: false
    tags:
      bill-to: databricks
      billed-project: c3
      env: dev
      

